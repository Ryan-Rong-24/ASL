{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0274c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10513cbf",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271765e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5440a",
   "metadata": {},
   "source": [
    "# 2. Keypoints using Mediapipe Hands and Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7329d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604f5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981047a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)    \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)    \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ebf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be653557",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoints Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965e9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(image,hands,pose):\n",
    "    results = hands.process(image)\n",
    "    results_pose = pose.process(image)\n",
    "\n",
    "    hand_np = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for i in range(len(results.multi_hand_landmarks)):\n",
    "            hand_np.append(np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i] else np.zeros(21*3))\n",
    "\n",
    "    if len(hand_np) == 0:\n",
    "        lh = np.zeros(21*3)\n",
    "        rh = np.zeros(21*3)\n",
    "    elif len(hand_np) == 1:\n",
    "        lh = hand_np[0]\n",
    "        rh = np.zeros(21*3)\n",
    "    else:\n",
    "        lh = hand_np[0]\n",
    "        rh = hand_np[1]        \n",
    "    ps = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "    return np.concatenate([ps, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d9627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_aug(image,hands,pose,xval,yval):\n",
    "    results = hands.process(image)\n",
    "    results_pose = pose.process(image)\n",
    "\n",
    "    hand_np = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for i in range(len(results.multi_hand_landmarks)):\n",
    "            hand_np.append(np.array([[res.x+xval, res.y+yval, res.z] if res.x != 0 and res.y!= 0 else [res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i].landmark else np.zeros(21*3))\n",
    "    if len(hand_np) == 0:\n",
    "        lh = np.zeros(21*3)\n",
    "        rh = np.zeros(21*3)\n",
    "    elif len(hand_np) == 1:\n",
    "        lh = hand_np[0]\n",
    "        rh = np.zeros(21*3)\n",
    "    else:\n",
    "        lh = hand_np[0]\n",
    "        rh = hand_np[1]    \n",
    "    ps = np.array([[res.x+xval, res.y+yval, res.z, res.visibility] if res.x != 0 and res.y!= 0 else [res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "    return np.concatenate([ps, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04ed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_flip_aug(image,hands,pose,xval,yval):\n",
    "    results = hands.process(image)\n",
    "    results_pose = pose.process(image)\n",
    "\n",
    "    hand_np = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for i in range(len(results.multi_hand_landmarks)):\n",
    "            hand_np.append(np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i] else np.zeros(21*3))\n",
    "    if len(hand_np) == 0:\n",
    "        lh1 = np.zeros(21*3)\n",
    "        rh1 = np.zeros(21*3)\n",
    "    elif len(hand_np) == 1:\n",
    "        lh1 = hand_np[0]\n",
    "        rh1 = np.zeros(21*3)\n",
    "    else:\n",
    "        lh1 = hand_np[0]\n",
    "        rh1 = hand_np[1]        \n",
    "    ps1 = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    hand_np = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for i in range(len(results.multi_hand_landmarks)):\n",
    "            hand_np.append(np.array([[res.x+xval, res.y+yval, res.z] if res.x != 0 and res.y!= 0 else [res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i].landmark else np.zeros(21*3))\n",
    "    if len(hand_np) == 0:\n",
    "        lh2 = np.zeros(21*3)\n",
    "        rh2 = np.zeros(21*3)\n",
    "    elif len(hand_np) == 1:\n",
    "        lh2 = hand_np[0]\n",
    "        rh2 = np.zeros(21*3)\n",
    "    else:\n",
    "        lh2 = hand_np[0]\n",
    "        rh2 = hand_np[1]    \n",
    "    ps2 = np.array([[res.x+xval, res.y+yval, res.z, res.visibility] if res.x != 0 and res.y!= 0 else [res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "    hand_np = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for i in range(len(results.multi_hand_landmarks)):\n",
    "            hand_np.append(np.array([[1-res.x, res.y, res.z] if res.x != 0 and res.y!= 0 else [res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i].landmark else np.zeros(21*3))\n",
    "    if len(hand_np) == 0:\n",
    "        lh3 = np.zeros(21*3)\n",
    "        rh3 = np.zeros(21*3)\n",
    "    elif len(hand_np) == 1:\n",
    "        lh3 = hand_np[0]\n",
    "        rh3 = np.zeros(21*3)\n",
    "    else:\n",
    "        lh3 = hand_np[0]\n",
    "        rh3 = hand_np[1]    \n",
    "    ps3 = np.array([[1-res.x, res.y, res.z, res.visibility] if res.x != 0 and res.y!= 0 else [res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "    \n",
    "    return np.concatenate([ps1, lh1, rh1]), np.concatenate([ps2, lh2, rh2]), np.concatenate([ps3, lh3, rh3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47b951ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2916612",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33bce163",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_aug = extract_keypoints_aug(results, random.uniform(0.01,0.05),random.uniform(0.01,0.05))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30700daf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53330a85",
   "metadata": {},
   "source": [
    "# 4. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a93ab2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MSASL') \n",
    "SAVE_PATH = os.path.join('keypoints_noface_msasl')\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "MAX_SEQ_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9531903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e701ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open(os.path.join(DATA_PATH,\"MSASL_classes.json\"),'r')\n",
    "data = [word.strip().replace('\"','').replace(\"[\",'').replace(\"]\",'').strip() for word in file.read().split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0483f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save action list into np array\n",
    "np.save('msasl_actions_list.npy',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c15f623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load action list from np array\n",
    "actions = np.load('msasl_actions_list.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "197193a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78d5175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_PATH,exist_ok=True)\n",
    "for action in actions:\n",
    "    os.makedirs(os.path.join(SAVE_PATH, action),exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68c288",
   "metadata": {},
   "source": [
    "# 5. Extract Keypoints & Augment for Collected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7627edd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe559dac775416ebe1e584673b95295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_file = json.load(open(os.path.join(DATA_PATH,\"MSASL_test.json\"),'r'))\n",
    "prev_id = \"\"\n",
    "num = 1\n",
    "for item in tqdm(data_file):\n",
    "#     if main_counter == 4:\n",
    "#         break\n",
    "    url_id = item[\"url\"][-11:]\n",
    "    label = actions[int(item[\"label\"])]\n",
    "    if url_id == prev_id:\n",
    "        num+=1\n",
    "    else:\n",
    "        num=1\n",
    "    if int(item[\"label\"]) < 100:\n",
    "        if os.path.exists(os.path.join(DATA_PATH, \"cropped_videos_testset\",url_id+str(num)+\".mp4\")):\n",
    "            if not os.path.exists(os.path.join(SAVE_PATH,label,url_id+str(num)+\"_0.npy\")):\n",
    "                cap = cv2.VideoCapture(os.path.join(DATA_PATH, \"cropped_videos_testset\",url_id+str(num)+\".mp4\"))\n",
    "                frame_count = 0\n",
    "                while(cap.isOpened()):\n",
    "                    # Capture frame-by-frame\n",
    "                    ret, image = cap.read()\n",
    "                    if ret == True:\n",
    "                        with mp_hands.Hands(model_complexity=0,min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n",
    "                            with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "\n",
    "                                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "                                image.flags.writeable = False\n",
    "\n",
    "                                # Make detections\n",
    "    #                             keypoints_flipped = extract_keypoints(cv2.flip(image,1), hands, pose)\n",
    "                                keypoints, keypoints_aug, keypoints_flipped = extract_keypoints_flip_aug(image, hands, pose,random.uniform(0.01,0.03),random.uniform(0.01,0.03))\n",
    "\n",
    "                                image.flags.writeable = True\n",
    "                                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                                # Export keypoints\n",
    "                                npy_path = os.path.join(SAVE_PATH,label,url_id+str(num)+\"_\"+str(frame_count)+\".npy\")\n",
    "                                np.save(npy_path, keypoints)\n",
    "\n",
    "                                npy_path_aug = os.path.join(SAVE_PATH,label,url_id+str(num)+\"_\"+str(frame_count)+\"_AUG.npy\")\n",
    "                                np.save(npy_path_aug, keypoints_aug)\n",
    "\n",
    "                                npy_path_flipped = os.path.join(SAVE_PATH,label,url_id+str(num)+\"_\"+str(frame_count)+\"_MIR.npy\")\n",
    "                                np.save(npy_path_flipped, keypoints_flipped)   \n",
    "                    else: \n",
    "                        break\n",
    "                    frame_count+=1\n",
    "        \n",
    "    prev_id = url_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b5e68",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9348f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b494dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0, 'nice': 1, 'teacher': 2, 'eat': 3, 'no': 4, 'happy': 5, 'like': 6, 'orange': 7, 'want': 8, 'deaf': 9, 'school': 10, 'sister': 11, 'finish': 12, 'white': 13, 'bird': 14, 'what': 15, 'tired': 16, 'friend': 17, 'sit': 18, 'mother': 19, 'yes': 20, 'student': 21, 'learn': 22, 'spring': 23, 'good': 24, 'fish': 25, 'again': 26, 'sad': 27, 'table': 28, 'need': 29, 'where': 30, 'father': 31, 'milk': 32, 'cousin': 33, 'brother': 34, 'paper': 35, 'forget': 36, 'nothing': 37, 'book': 38, 'girl': 39, 'fine': 40, 'black': 41, 'boy': 42, 'lost': 43, 'family': 44, 'hearing': 45, 'bored': 46, 'please': 47, 'water': 48, 'computer': 49, 'help': 50, 'doctor': 51, 'yellow': 52, 'write': 53, 'hungry': 54, 'but': 55, 'drink': 56, 'bathroom': 57, 'man': 58, 'how': 59, 'understand': 60, 'red': 61, 'beautiful': 62, 'sick': 63, 'blue': 64, 'green': 65, 'english': 66, 'name': 67, 'you': 68, 'who': 69, 'same': 70, 'nurse': 71, 'day': 72, 'now': 73, 'brown': 74, 'thanks': 75, 'hurt': 76, 'here': 77, 'grandmother': 78, 'pencil': 79, 'walk': 80, 'bad': 81, 'read': 82, 'when': 83, 'dance': 84, 'play': 85, 'sign': 86, 'go': 87, 'big': 88, 'sorry': 89, 'work': 90, 'draw': 91, 'grandfather': 92, 'woman': 93, 'right': 94, 'france': 95, 'pink': 96, 'know': 97, 'live': 98, 'night': 99}\n"
     ]
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions[:100])}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0cebaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85def51c362c4f749f060f6d7376637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "for action in tqdm(label_map.keys()):\n",
    "    if  os.path.exists(os.path.join(SAVE_PATH, action)):\n",
    "#         for sequence in range(1,no_sequences+1):\n",
    "        sequence_list = np.array([name[:12] + name[12:].split(\"_\")[0] for name in os.listdir(os.path.join(SAVE_PATH, action))])\n",
    "#         sequence_map = {}\n",
    "#         for sequence in np.unique(sequence_list):\n",
    "#             sequence_map[sequence] = np.count_nonzero(sequence_list == sequence)//3\n",
    "#             maxLength = max(maxLength,sequence_map[sequence])\n",
    "#             minLength = min(minLength,sequence_map[sequence]) \n",
    "        for sequence in np.unique(sequence_list):\n",
    "            window = []\n",
    "            last_frame = 0\n",
    "#             for frame_num in range(0,sequence_map[sequence]):\n",
    "            for frame_num in range(0,MAX_SEQ_LENGTH):\n",
    "\n",
    "                if  os.path.exists(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}.npy\".format(frame_num))): \n",
    "                    res = np.load(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}.npy\".format(frame_num)))\n",
    "                    last_frame = frame_num\n",
    "                else: # if the video length is shorter than sequence length, need to pad\n",
    "                    res = np.zeros((258,),dtype='float64') # zero padding\n",
    "#                     res = np.load(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}.npy\".format(last_frame))) # last frame padding\n",
    "                window.append(res)\n",
    "\n",
    "            # repeat for aug and mir\n",
    "            window_aug = []\n",
    "            last_frame = 0\n",
    "            for frame_num in range(0,MAX_SEQ_LENGTH):\n",
    "                if  os.path.exists(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}_AUG.npy\".format(frame_num))): \n",
    "                    res = np.load(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}_AUG.npy\".format(frame_num)))\n",
    "                    last_frame = frame_num\n",
    "                else: \n",
    "                    res = np.zeros((258,),dtype='float64') # zero padding\n",
    "#                     res = np.load(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}_AUG.npy\".format(last_frame))) # last frame padding\n",
    "                window_aug.append(res)\n",
    "                       \n",
    "            window_mir = []\n",
    "            last_frame = 0\n",
    "            for frame_num in range(0,MAX_SEQ_LENGTH):\n",
    "                if  os.path.exists(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}_MIR.npy\".format(frame_num))): \n",
    "                    res = np.load(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}_MIR.npy\".format(frame_num)))\n",
    "                    last_frame = frame_num\n",
    "                else: \n",
    "                    res = np.zeros((258,),dtype='float64') # zero padding\n",
    "#                     res = np.load(os.path.join(SAVE_PATH, action, sequence+\"_\"+\"{}_MIR.npy\".format(last_frame))) # last frame padding\n",
    "                window_mir.append(res)\n",
    "\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "            sequences.append(window_aug)\n",
    "            labels.append(label_map[action])\n",
    "            sequences.append(window_mir)\n",
    "            labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd8391d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bedf9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10173, 256, 258)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18452d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels,num_classes=len(actions[:100])).astype(int)\n",
    "y = np.argmax(y, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27eb5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [len(y[y==i]) for i in range(len(actions[:100]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faf134cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aae1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [len(y_test[y_test==i]) for i in range(len(actions[:100]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9ae73fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1b1d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"transformer_data_\"+\"msasl\",exist_ok=True)\n",
    "np.save(os.path.join(\"transformer_data_\"+\"msasl\",\"train_data_new.npy\"),X_train)\n",
    "np.save(os.path.join(\"transformer_data_\"+\"msasl\",\"train_labels_new.npy\"),y_train)\n",
    "np.save(os.path.join(\"transformer_data_\"+\"msasl\",\"test_data_new.npy\"),X_test)\n",
    "np.save(os.path.join(\"transformer_data_\"+\"msasl\",\"test_labels_new.npy\"),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adf8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
