{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.callbacks import TensorBoard\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import cv2\r\n",
    "import os\r\n",
    "import time\r\n",
    "import json\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "MAX_SEQ_LENGTH = 256\r\n",
    "MAX_SEQ_LENGTH = 30\r\n",
    "NUM_FEATURES = 258\r\n",
    "\r\n",
    "EPOCHS = 200\r\n",
    "\r\n",
    "# DATA_PATH = \"transformer_datanoface_msasl1000\"\r\n",
    "DATA_PATH = \"transformer_datanoface_seq60_16actions_algostruc1\"\r\n",
    "\r\n",
    "\r\n",
    "# actions = np.load('msasl_actions_list.npy')[:100]\r\n",
    "# actions = json.load(open(os.path.join(\"MSASL\",\"MSASL_classes.json\")))\r\n",
    "# actions.sort()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# actions = ['can','help','me','water']\r\n",
    "actions = ['can','you','help','me','your','name','what','hello','bye','excuse me','sorry','water','thanks','yesterday','yes','no','family','clothe','mom','dad'] # seq20\r\n",
    "# actions = ['can','you','help','me','your','name','what','hamburger','french fries','hello','bye','excuse me','sorry','water','thanks','yesterday','family','clothe','yes','no']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "len(actions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# train_data, train_labels = np.load(os.path.join(DATA_PATH,\"X_train_split.npy\"),allow_pickle=True), np.load(os.path.join(DATA_PATH,\"y_train_split.npy\"),allow_pickle=True)\r\n",
    "# test_data, test_labels = np.load(os.path.join(DATA_PATH,\"X_test_split.npy\"),allow_pickle=True), np.load(os.path.join(DATA_PATH,\"y_test_split.npy\"),allow_pickle=True)\r\n",
    "\r\n",
    "# print(f\"Frame features in train set: {train_data.shape}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "train_data, train_labels = np.load(os.path.join(DATA_PATH,\"train_data_30fps_seq20.npy\")), np.load(os.path.join(DATA_PATH,\"train_labels_30fps_seq20.npy\"))\r\n",
    "test_data, test_labels = np.load(os.path.join(DATA_PATH,\"test_data_30fps_seq20.npy\")), np.load(os.path.join(DATA_PATH,\"test_labels_30fps_seq20.npy\"))\r\n",
    "\r\n",
    "print(f\"Frame features in train set: {train_data.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Frame features in train set: (4911, 30, 258)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "assert not np.any(np.isnan(train_data))\r\n",
    "assert not np.any(np.isnan(train_labels))\r\n",
    "assert not np.any(np.isnan(test_data))\r\n",
    "assert not np.any(np.isnan(test_labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Transformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class PositionalEmbedding(layers.Layer):\r\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.position_embeddings = layers.Embedding(\r\n",
    "            input_dim=sequence_length, output_dim=output_dim\r\n",
    "        )\r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\r\n",
    "        length = tf.shape(inputs)[1]\r\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\r\n",
    "        embedded_positions = self.position_embeddings(positions)\r\n",
    "        return inputs + embedded_positions\r\n",
    "\r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\r\n",
    "        return mask\r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "#             \"position_embeddings\": self.position_embeddings,\r\n",
    "            \"sequence_length\": self.sequence_length,\r\n",
    "            \"output_dim\": self.output_dim,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class TransformerEncoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.dense_dim = dense_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        self.attention = layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\r\n",
    "        )\r\n",
    "        self.dense_proj = keras.Sequential(\r\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\r\n",
    "        )\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()\r\n",
    "\r\n",
    "    def call(self, inputs, mask=None):\r\n",
    "        if mask is not None:\r\n",
    "            mask = mask[:, tf.newaxis, :]\r\n",
    "\r\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\r\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\r\n",
    "        proj_output = self.dense_proj(proj_input)\r\n",
    "        return self.layernorm_2(proj_input + proj_output)\r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "#             \"attention\": self.attention,\r\n",
    "#             \"dense_proj\": self.dense_proj,\r\n",
    "#             \"layernorm_1\": self.layernorm_1,\r\n",
    "#             \"layernorm_2\": self.layernorm_2,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def get_compiled_model():\r\n",
    "    sequence_length = MAX_SEQ_LENGTH\r\n",
    "    embed_dim = NUM_FEATURES\r\n",
    "    dense_dim = 8\r\n",
    "    num_heads = 6\r\n",
    "    classes = len(actions)\r\n",
    "\r\n",
    "    inputs = keras.Input(shape=(None, None))\r\n",
    "    x = PositionalEmbedding(\r\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\r\n",
    "    )(inputs)\r\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\r\n",
    "    x = layers.GlobalMaxPooling1D()(x)\r\n",
    "    x = layers.Dropout(0.5)(x)\r\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\r\n",
    "    model = keras.Model(inputs, outputs)\r\n",
    "\r\n",
    "    model.compile(\r\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\r\n",
    "    )\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def run_experiment(filepath):\r\n",
    "#     checkpoint = keras.callbacks.ModelCheckpoint(\r\n",
    "#         filepath, save_weights_only=True, save_best_only=True, verbose=1\r\n",
    "#     )\r\n",
    "    checkpoint = TensorBoard(log_dir=filepath)\r\n",
    "    model = get_compiled_model()\r\n",
    "    history = model.fit(\r\n",
    "        train_data,\r\n",
    "        train_labels,\r\n",
    "        validation_split=0.15,\r\n",
    "        epochs=EPOCHS,\r\n",
    "        callbacks=[checkpoint],\r\n",
    "    )\r\n",
    "\r\n",
    "    model.load_weights(filepath)\r\n",
    "    _, accuracy = model.evaluate(test_data, test_labels)\r\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def load_model(filepath):\r\n",
    "    model = get_compiled_model()\r\n",
    "    model.load_weights(filepath)\r\n",
    "    _, accuracy = model.evaluate(test_data, test_labels)\r\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\r\n",
    "\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load or Train Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "trained_model = run_experiment(\"transformer_model_noface_msasl1000/model\") # arg: save path"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 10s 34ms/step - loss: 2.0156 - accuracy: 0.3210 - val_loss: 0.9723 - val_accuracy: 0.6242\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 1.1438 - accuracy: 0.5244 - val_loss: 0.8271 - val_accuracy: 0.6376\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.8277 - accuracy: 0.6671 - val_loss: 0.6762 - val_accuracy: 0.7718\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.7051 - accuracy: 0.7289 - val_loss: 0.4591 - val_accuracy: 0.8591\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.4684 - accuracy: 0.8157 - val_loss: 0.6100 - val_accuracy: 0.7919\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.4320 - accuracy: 0.8240 - val_loss: 0.3256 - val_accuracy: 0.9128\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3178 - accuracy: 0.8918 - val_loss: 0.3104 - val_accuracy: 0.9060\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.2612 - accuracy: 0.9061 - val_loss: 0.3126 - val_accuracy: 0.9060\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.3016 - accuracy: 0.8835 - val_loss: 0.3438 - val_accuracy: 0.8792\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2828 - accuracy: 0.9037 - val_loss: 0.2795 - val_accuracy: 0.8993\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.1904 - accuracy: 0.9263 - val_loss: 0.4585 - val_accuracy: 0.8792\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1534 - accuracy: 0.9417 - val_loss: 0.2370 - val_accuracy: 0.9396\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1652 - accuracy: 0.9358 - val_loss: 0.2745 - val_accuracy: 0.9128\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1580 - accuracy: 0.9405 - val_loss: 0.2588 - val_accuracy: 0.9329\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1462 - accuracy: 0.9465 - val_loss: 0.2745 - val_accuracy: 0.9329\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.1156 - accuracy: 0.9667 - val_loss: 0.3010 - val_accuracy: 0.9262\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 0.1648 - accuracy: 0.9429 - val_loss: 0.3984 - val_accuracy: 0.8792\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1532 - accuracy: 0.9370 - val_loss: 0.3807 - val_accuracy: 0.8993\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1358 - accuracy: 0.9489 - val_loss: 0.2404 - val_accuracy: 0.9262\n",
      "Epoch 20/200\n",
      " 1/27 [>.............................] - ETA: 0s - loss: 0.1767 - accuracy: 0.9375"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer_model_noface_msasl1000/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     28\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m TensorBoard(log_dir\u001b[38;5;241m=\u001b[39mfilepath)\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m get_compiled_model()\n\u001b[1;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(filepath)\n\u001b[0;32m     39\u001b[0m _, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_data, test_labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1387\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1389\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1391\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    555\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 557\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "trained_model = load_model(\"transformer_model_noface_action20/model\") # arg: load path"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18/18 [==============================] - 4s 19ms/step - loss: 9.0751 - accuracy: 0.4744\n",
      "Test accuracy: 47.44%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "del trained_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run it real time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import time\r\n",
    "import mediapipe as mp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "mp_hands = mp.solutions.hands\r\n",
    "mp_pose = mp.solutions.pose\r\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utils\r\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def extract_keypoints(results):\r\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\r\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\r\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\r\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\r\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def extract_keypoints(image,hands,pose):\r\n",
    "    results = hands.process(image)\r\n",
    "    results_pose = pose.process(image)\r\n",
    "\r\n",
    "    hand_np = []\r\n",
    "    if results.multi_hand_landmarks:\r\n",
    "        for i in range(len(results.multi_hand_landmarks)):\r\n",
    "            hand_np.append(np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i] else np.zeros(21*3))\r\n",
    "\r\n",
    "    if len(hand_np) == 0:\r\n",
    "        lh = np.zeros(21*3)\r\n",
    "        rh = np.zeros(21*3)\r\n",
    "    elif len(hand_np) == 1:\r\n",
    "        lh = hand_np[0]\r\n",
    "        rh = np.zeros(21*3)\r\n",
    "    else:\r\n",
    "        lh = hand_np[0]\r\n",
    "        rh = hand_np[1]        \r\n",
    "    ps = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\r\n",
    "\r\n",
    "    return np.concatenate([ps, lh, rh])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\r\n",
    "def prob_viz(res, actions, input_frame, colors):\r\n",
    "    output_frame = input_frame.copy()\r\n",
    "    for num, prob in enumerate(res):\r\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num%3], -1)\r\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\r\n",
    "        \r\n",
    "    return output_frame"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# 1. New detection variables\r\n",
    "sequence = []\r\n",
    "sentence = []\r\n",
    "predictions = []\r\n",
    "threshold = 0.7\r\n",
    "\r\n",
    "cap = cv2.VideoCapture(0)\r\n",
    "# Set mediapipe model \r\n",
    "with mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\r\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\r\n",
    "        while cap.isOpened():\r\n",
    "\r\n",
    "            # Read feed\r\n",
    "            ret, image = cap.read()\r\n",
    "\r\n",
    "            # To improve performance, optionally mark the image as not writeable to\r\n",
    "            # pass by reference.\r\n",
    "            image.flags.writeable = False\r\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n",
    "            results = hands.process(image)\r\n",
    "            results_pose = pose.process(image)\r\n",
    "\r\n",
    "\r\n",
    "            # Draw the hand annotations on the image.\r\n",
    "            image.flags.writeable = True\r\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #             print(len(results.multi_hand_landmarks))\r\n",
    "            hand_np = []\r\n",
    "            if results.multi_hand_landmarks:\r\n",
    "                for i in range(len(results.multi_hand_landmarks)):\r\n",
    "#                     print(i)\r\n",
    "                    mp_drawing.draw_landmarks(image,results.multi_hand_landmarks[i],mp_hands.HAND_CONNECTIONS,mp_drawing_styles.get_default_hand_landmarks_style(),mp_drawing_styles.get_default_hand_connections_style())\r\n",
    "                    hand_np.append(np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[i].landmark]).flatten() if results.multi_hand_landmarks[i] else np.zeros(21*3))\r\n",
    "\r\n",
    "            if len(hand_np) == 0:\r\n",
    "                lh = np.zeros(21*3)\r\n",
    "                rh = np.zeros(21*3)\r\n",
    "            elif len(hand_np) == 1:\r\n",
    "                lh = hand_np[0]\r\n",
    "                rh = np.zeros(21*3)\r\n",
    "            else:\r\n",
    "                lh = hand_np[0]\r\n",
    "                rh = hand_np[1]        \r\n",
    "\r\n",
    "            # Draw pose annotations        \r\n",
    "            mp_drawing.draw_landmarks(image,results_pose.pose_landmarks,mp_pose.POSE_CONNECTIONS,landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\r\n",
    "\r\n",
    "            ps = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\r\n",
    "\r\n",
    "            keypoints = np.concatenate([ps, lh, rh])\r\n",
    "            sequence.append(keypoints)\r\n",
    "            sequence = sequence[-sequence_length:]\r\n",
    "\r\n",
    "            if len(sequence) == sequence_length:\r\n",
    "                res = trained_model.predict(np.expand_dims(sequence, axis=0))[0]\r\n",
    "#                 print(actions[np.argmax(res)])\r\n",
    "                predictions.append(np.argmax(res))\r\n",
    "\r\n",
    "\r\n",
    "                #3. Viz logic\r\n",
    "                if np.unique(predictions[-10:])[0]==np.argmax(res): \r\n",
    "                    if res[np.argmax(res)] > threshold: \r\n",
    "\r\n",
    "                        if len(sentence) > 0: \r\n",
    "                            if actions[np.argmax(res)] != sentence[-1]:\r\n",
    "                                sentence.append(actions[np.argmax(res)])\r\n",
    "                        else:\r\n",
    "                            sentence.append(actions[np.argmax(res)])\r\n",
    "\r\n",
    "                if len(sentence) > 4: \r\n",
    "                    sentence = sentence[-4:]\r\n",
    "\r\n",
    "                # Viz probabilities\r\n",
    "                image = prob_viz(res, actions, image, colors)\r\n",
    "                \r\n",
    "                sequence = []\r\n",
    "                cv2.waitKey(3000)\r\n",
    "                \r\n",
    "\r\n",
    "            cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\r\n",
    "            cv2.putText(image, ' '.join(sentence), (3,30), \r\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "\r\n",
    "            # Show to screen\r\n",
    "            cv2.imshow('OpenCV Feed', image)\r\n",
    "\r\n",
    "            # Break gracefully\r\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\r\n",
    "                break\r\n",
    "        cap.release()\r\n",
    "        cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sequence_length' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([ps, lh, rh])\n\u001b[0;32m     54\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(keypoints)\n\u001b[1;32m---> 55\u001b[0m sequence \u001b[38;5;241m=\u001b[39m sequence[\u001b[38;5;241m-\u001b[39m\u001b[43msequence_length\u001b[49m:]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;241m==\u001b[39m sequence_length:\n\u001b[0;32m     58\u001b[0m     res \u001b[38;5;241m=\u001b[39m trained_model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(sequence, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sequence_length' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# 1. New detection variables\r\n",
    "images = []\r\n",
    "count = 0\r\n",
    "num_of_vid = 0\r\n",
    "threshold = 0.7\r\n",
    "\r\n",
    "cap = cv2.VideoCapture(0)\r\n",
    "while cap.isOpened():\r\n",
    "    # Read feed\r\n",
    "    ret, image = cap.read()\r\n",
    "    \r\n",
    "    if count == 0:\r\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\r\n",
    "        cv2.putText(image, ' '.join(\"START\"), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "        cv2.waitKey(1000)\r\n",
    "\r\n",
    "    if count%2==1:\r\n",
    "        images.append(image)\r\n",
    "        \r\n",
    "    count+=1\r\n",
    "\r\n",
    "    if count == 61:\r\n",
    "        num_of_vid+=1\r\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\r\n",
    "        cv2.putText(image, ' '.join(\"THINKING\"), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "\r\n",
    "    cv2.imshow('OpenCV Feed', image)\r\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\r\n",
    "        break\r\n",
    "    if count == 61:\r\n",
    "        sequence = []\r\n",
    "        tic = time.perf_counter()\r\n",
    "        with mp_hands.Hands(min_detection_confidence=0.3,min_tracking_confidence=0.5) as hands:\r\n",
    "            with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\r\n",
    "                for image in images:\r\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \r\n",
    "                    image.flags.writeable = False\r\n",
    "\r\n",
    "                    # Make detections\r\n",
    "                    keypoints = extract_keypoints(image, hands, pose)\r\n",
    "                    \r\n",
    "                    sequence.append(keypoints)\r\n",
    "                    \r\n",
    "        res = trained_model.predict(np.expand_dims(sequence, axis=0))[0]\r\n",
    "        print(actions[np.argmax(res)])\r\n",
    "        toc = time.perf_counter()\r\n",
    "        print(f\"Predictions took {toc-tic} seconds\")\r\n",
    "        count = 0\r\n",
    "        images = []\r\n",
    "        \r\n",
    "cap.release()\r\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "me\n",
      "Predictions took 4.623992600012571 seconds\n",
      "me\n",
      "Predictions took 2.828193800058216 seconds\n",
      "name\n",
      "Predictions took 2.4940170999616385 seconds\n",
      "me\n",
      "Predictions took 2.7080847998149693 seconds\n",
      "me\n",
      "Predictions took 2.3155856002122164 seconds\n",
      "no\n",
      "Predictions took 2.9190536998212337 seconds\n",
      "excuse me\n",
      "Predictions took 2.5746996998786926 seconds\n",
      "yes\n",
      "Predictions took 2.510443899780512 seconds\n",
      "me\n",
      "Predictions took 2.604802300222218 seconds\n",
      "yes\n",
      "Predictions took 2.531889899633825 seconds\n",
      "water\n",
      "Predictions took 2.725152399856597 seconds\n",
      "me\n",
      "Predictions took 2.353437200188637 seconds\n",
      "help\n",
      "Predictions took 2.7672684998251498 seconds\n",
      "me\n",
      "Predictions took 2.6995709999464452 seconds\n",
      "name\n",
      "Predictions took 3.15169549966231 seconds\n",
      "no\n",
      "Predictions took 2.5381605001166463 seconds\n",
      "me\n",
      "Predictions took 2.2801121999509633 seconds\n",
      "no\n",
      "Predictions took 2.6793233002536 seconds\n",
      "me\n",
      "Predictions took 2.527946399990469 seconds\n",
      "me\n",
      "Predictions took 2.7434930000454187 seconds\n",
      "me\n",
      "Predictions took 2.404556500259787 seconds\n",
      "me\n",
      "Predictions took 2.339322399813682 seconds\n",
      "me\n",
      "Predictions took 2.0786606003530324 seconds\n",
      "me\n",
      "Predictions took 2.0234415996819735 seconds\n",
      "me\n",
      "Predictions took 2.0105290999636054 seconds\n",
      "me\n",
      "Predictions took 2.178516599815339 seconds\n",
      "me\n",
      "Predictions took 2.186547600198537 seconds\n",
      "me\n",
      "Predictions took 1.9888872001320124 seconds\n",
      "yesterday\n",
      "Predictions took 2.0389272002503276 seconds\n",
      "yesterday\n",
      "Predictions took 1.9975223997607827 seconds\n",
      "yesterday\n",
      "Predictions took 1.9907932002097368 seconds\n",
      "me\n",
      "Predictions took 1.9982911995612085 seconds\n",
      "me\n",
      "Predictions took 2.0281622000038624 seconds\n",
      "me\n",
      "Predictions took 2.0156519999727607 seconds\n",
      "me\n",
      "Predictions took 2.0240567000582814 seconds\n",
      "yesterday\n",
      "Predictions took 2.0603781999088824 seconds\n",
      "me\n",
      "Predictions took 2.0441665002144873 seconds\n",
      "me\n",
      "Predictions took 2.0337861999869347 seconds\n",
      "me\n",
      "Predictions took 2.1169090000912547 seconds\n",
      "me\n",
      "Predictions took 2.0300310999155045 seconds\n",
      "me\n",
      "Predictions took 2.087650700006634 seconds\n",
      "me\n",
      "Predictions took 2.000763399992138 seconds\n",
      "me\n",
      "Predictions took 2.0522508998401463 seconds\n",
      "me\n",
      "Predictions took 2.0172992004081607 seconds\n",
      "me\n",
      "Predictions took 2.046422100160271 seconds\n",
      "yesterday\n",
      "Predictions took 2.0371966999955475 seconds\n",
      "me\n",
      "Predictions took 2.0293748998083174 seconds\n",
      "me\n",
      "Predictions took 2.011936299968511 seconds\n",
      "name\n",
      "Predictions took 2.407775799743831 seconds\n",
      "me\n",
      "Predictions took 2.4414309998974204 seconds\n",
      "thanks\n",
      "Predictions took 2.373320300132036 seconds\n",
      "you\n",
      "Predictions took 2.456479699816555 seconds\n",
      "me\n",
      "Predictions took 2.4685428999364376 seconds\n",
      "me\n",
      "Predictions took 2.0465788999572396 seconds\n",
      "me\n",
      "Predictions took 2.0746772000566125 seconds\n",
      "me\n",
      "Predictions took 2.1686859000474215 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clothe\n",
      "Predictions took 2.818778600078076 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "interpreter": {
   "hash": "b62186588bf472f5abc3fe199a5e5d185fd4dc91c41e87cceaa93b25f13751eb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}